{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qUzaaWkb0J3"
      },
      "source": [
        "# CS145 Introduction to Data Mining - Assignment 3\n",
        "## Deadline: 11:59PM, May 5, 2025\n",
        "\n",
        "## Instructions\n",
        "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
        "\n",
        "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
        "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
        "\n",
        "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
        "\n",
        "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as x0, x^1, or R x Q for equations.\n",
        "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
        "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
        "\n",
        "### Submission Requirements\n",
        "\n",
        "* Submit your solutions in .ipynb format through GradeScope in BruinLearn.\n",
        "* Late submissions are allowed up to 24 hours post-deadline with a penalty factor of $\\mathbf{1}(t\\leq24)e^{-(\\ln(2)/12)t}$.\n",
        "\n",
        "### Collaboration and Integrity\n",
        "\n",
        "* High level discussions are allowed and encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
        "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOMabQY3b0J5"
      },
      "source": [
        "## Part 1: Write-Up Questions\n",
        "\n",
        "### 1. Neural Network Derivatives (12 points total)\n",
        "\n",
        "\n",
        "In this problem, you will analyze the effect of ReLU activation on weight gradients and derive gradients for a softmax cross-entropy loss. **Answer each sub-question carefully with full justification.**\n",
        "\n",
        "#### (a) ReLU Derivatives (6 points)\n",
        "\n",
        "> Definition.\n",
        ">\n",
        "> $$\n",
        "> \\operatorname{ReLU}(x)=\\max (0, x), \\quad \\operatorname{ReLU}^{\\prime}(x)= \\begin{cases}1, & x>0 \\\\ 0, & x \\leq 0\\end{cases}\n",
        "> $$\n",
        ">\n",
        ">\n",
        "> Hint. When a unit's input is non-positive, its ReLU output is zero and \"turns off\" all gradient flow through that unit.\n",
        ">\n",
        "![image-20250420170015320](https://drive.google.com/uc?id=1X0Est80T0gm1N2VrYq8MzmfdoxcdtAjR)\n",
        "\n",
        "Consider a neural network with the following structure:\n",
        "- **Input layer**: $ x_1, x_2 $\n",
        "- **Hidden layer 1**: $ h_3, h_4 $ with ReLU activation\n",
        "- **Hidden layer 2**: $ h_1, h_2 $ with ReLU activation\n",
        "- **Output layer**: $\\hat{y}$\n",
        "\n",
        "The weights $w_1, w_2, \\ldots, w_5$ connect individual units as shown in a diagram, and we aim to minimize a loss function $L$ depending only on the output $\\hat{y}$. Suppose one ReLU unit $h_1$ is “inactive” (its input is negative, so its output is 0).  \n",
        "\n",
        "1. Which partial derivatives\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial w_1}, \\quad \\frac{\\partial L}{\\partial w_2}, \\quad \\frac{\\partial L}{\\partial w_3}\n",
        "   $$\n",
        "   are guaranteed to be zero?  \n",
        "2. **Justify** your answer by explaining how the ReLU activation cuts off gradients for inactive units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ9JUqd6b0J5"
      },
      "source": [
        "**If $h_1$ is inactive, then  partial derivatives $\n",
        "   \\frac{\\partial L}{\\partial w_1}= \\quad \\frac{\\partial L}{\\partial w_2}\n",
        "   = 0$ because, the derivative of ReLU is 0 for any input $x\\leq0$ so if $h_1 < 0$ then since $w_1$ feeds out of $h_1$, it will be negative, and thus 0. Since $w_2$ is an input into $h_1$, it will also be zero since $h_1$ is blocked/dropped, so that weight will not contribute to the final loss (this is because $w_2$ is only connected to $h_1$ and doesn't affect the value of any other neuron. This is not true for $w_3$, because it contributes to the value of $h_3$, which thus connects/contributes to the value of $h_2$ as well as $h_1$, so even though $h_1$ is blocked, this weight still affects the final loss value through $h_2$, so this partial will be nonzero.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plw18jZBb0J5"
      },
      "source": [
        "#### (b) Cross-Entropy Loss: Chain Rule Derivation (6 points)\n",
        "\n",
        "> We now replace the softmax/matrix formulation with a single‑neuron network using sigmoid + BCE.\n",
        "\n",
        "Consider a neural network with:\n",
        "- Inputs: $x_1, x_2$\n",
        "- Output neuron $z=w_1 x_1+w_2 x_2+b$\n",
        "- Activation: $\\hat{y}=\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
        "- True label: $y \\in\\{0,1\\}$\n",
        "- Loss:\n",
        "\n",
        "$$\n",
        "L=-[y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})] \\quad \\text { (binary cross-entropy) }\n",
        "$$\n",
        "\n",
        "\n",
        "**Task.** Derive expressions for\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1}, \\quad \\frac{\\partial L}{\\partial w_2}, \\quad \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "by unfolding the chain rule through the sigmoid activation and the BCE loss.\n",
        "Be sure to show each step:\n",
        "1. $\\partial L / \\partial \\hat{y}$\n",
        "2. $\\partial \\hat{y} / \\partial z$\n",
        "3. $\\partial z / \\partial w_i$ and $\\partial z / \\partial b$\n",
        "4. Combine to get $\\partial L / \\partial w_i, \\partial L / \\partial b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReYp4ULb0J5"
      },
      "source": [
        "**For each of these, we will use the chain rule**, so $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat y} * \\frac{\\partial \\hat y}{\\partial z} * \\frac{\\partial z}{\\partial w_1}$ and similarly\n",
        "$\n",
        "\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat y} * \\frac{\\partial \\hat y}{\\partial z} * \\frac{\\partial z}{\\partial w_2}\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat y} * \\frac{\\partial \\hat y}{\\partial z} * \\frac{\\partial z}{\\partial b}\n",
        "$\n",
        "\n",
        "We will first start by calculating $\\frac{\\partial L}{\\partial \\hat y} = -\\frac{y}{\\hat y} + \\frac{1-y}{1-\\hat y}$\n",
        "\n",
        "Now we will calculate $\\frac{\\partial \\hat y}{\\partial z}$, remembering that $\\hat y = \\sigma(z)$ and we know that $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$ which means that $\\frac{\\partial \\hat y}{\\partial z} = \\hat y(1 - \\hat y)$ if you substite the value for y back into the derivative.\n",
        "\n",
        "Now, remeber that $z= w_1x_1 + w_2x_2 + b$, so we will take the partial derivative of this equation to get $\\frac{\\partial z}{\\partial w_1} = x_1, \\frac{\\partial z}{\\partial w_2} = x_2, \\frac{\\partial z}{\\partial b} = 1$\n",
        "\n",
        "Since we used the chain rule, we now have to combine all of these products to get\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = (-\\frac{y}{\\hat y} + \\frac{1-y}{1-\\hat y}) * (\\hat y(1 - \\hat y))*(x_1) \\quad\n",
        "\\frac{\\partial L}{\\partial w_2} = (-\\frac{y}{\\hat y} + \\frac{1-y}{1-\\hat y}) * (\\hat y(1 - \\hat y))*(x_2) \\quad\n",
        "\\frac{\\partial L}{\\partial b} = (-\\frac{y}{\\hat y} + \\frac{1-y}{1-\\hat y}) * (\\hat y(1 - \\hat y))*1\n",
        "$$\n",
        "\n",
        "And we can further simplify that to get\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = (\\hat y - y) x_1 \\quad\n",
        "\\frac{\\partial L}{\\partial w_2} = (\\hat y - y) x_2 \\quad\n",
        "\\frac{\\partial L}{\\partial b} = (\\hat y - y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlYSUq-b0J6"
      },
      "source": [
        "### 2. Two-Layer MLP for XOR (6 points)\n",
        "\n",
        "We know a single-layer perceptron cannot represent the XOR function. A **two-layer network** with an appropriate choice of weights and biases can solve it.  \n",
        "\n",
        "1. **Construct** such a two-layer MLP (with a single hidden layer) that outputs 1 for XOR=1 and 0 for XOR=0, given inputs $\\{(x_1, x_2)\\mid x_1,x_2 \\in \\{0,1\\}\\}$.  \n",
        "   - Specify your network’s architecture (size of hidden layer, activation function, final layer).\n",
        "   - Provide **explicit** weight and bias values.  \n",
        "2. **Demonstrate** that for all $(x_1,x_2)$ in $\\{0,1\\}\\times\\{0,1\\}$, the final output is correct (0 or 1) for XOR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GEWiia0b0J6"
      },
      "source": [
        "**My network would have two neurons in the input layer, two neurons in the hidden layer, and one neuron in the output layer. It would use the step activation function and the weights $$\n",
        "\\mathbf{w_1} = \\begin{bmatrix}\n",
        "-1 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix},\\quad \\mathbf{w_2} = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "-1 \\\\\n",
        "\\end{bmatrix}, \\quad \\mathbf{w_2} = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$ with w1 connecting input neuron 1 to the hidden layer, w2 connecting the second input neuron to the hidden layer, and w3 connecting the hidden layer neurons to the output. The bias value in this example would be 0**\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1URZruKZQ_9U4zb4qr8AxxdLIKbJvP56e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyn-Krj9b0J6"
      },
      "source": [
        "### 3. $K$-Means Clustering (8 points)\n",
        "\n",
        "![image-20250420170622686](https://drive.google.com/uc?id=1SUOHENOHHkgQvRzSI3FIvaZF37c4wPuQ)\n",
        "\n",
        "Recall the following data points in $\\mathbb{R}^2$:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "5.9 & 3.2 \\\\\n",
        "4.6 & 2.9 \\\\\n",
        "6.2 & 2.8 \\\\\n",
        "4.7 & 3.2 \\\\\n",
        "5.5 & 4.2 \\\\\n",
        "5.0 & 3.0 \\\\\n",
        "4.9 & 3.1 \\\\\n",
        "6.7 & 3.1 \\\\\n",
        "5.1 & 3.8 \\\\\n",
        "6.0 & 3.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We apply $K$-Means with $K=3$ using Euclidean distance, starting from cluster centers $\\boldsymbol{\\mu}_1=(6.2,3.2), \\boldsymbol{\\mu}_2=(6.6,3.7), \\boldsymbol{\\mu}_3=(6.5,3.0)$.\n",
        "\n",
        "1. **(2 pts)** Calculate the center of Cluster 1 after one iteration. Show your intermediate step of assigning points and then compute the updated mean.\n",
        "2. **(2 pts)** Compute the center of Cluster 2 after **two** iterations (round to three decimals).\n",
        "3. **(2 pts)** Find the center of Cluster 3 at convergence (round to three decimals).\n",
        "4. **(2 pts)** How many iterations does it take to converge (no changes in cluster assignments)?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. I first calculated all of the points' distances to each of the three centers using Euclidean distance and made the following assignments: [5.9 3.2] to c1,\n",
        "[4.6 2.9] to c1,\n",
        "[6.2 2.8] to c3,\n",
        "[4.7 3.2] to c1,\n",
        "[5.5 4.2] to c2,\n",
        "[5. 3.] to c1,\n",
        "[4.9 3.1] to c1,\n",
        "[6.7 3.1] to c3\n",
        "[5.1 3.8] to c1,\n",
        "[6. 3.] to c1\n",
        "\n",
        "Now cluster one has points [5.9 3.2] to c1,\n",
        "[4.6 2.9] to c1, [4.7 3.2] to c1, [5. 3.] to c1,\n",
        "[4.9 3.1] to c1, [5.1 3.8] to c1, [6. 3.] to c1, making the new $\\mu_1 = (5.171, 3.171)$\n",
        "\n",
        "2. After 1 iteration, Cluster 2's mean became [5.5, 4.2], and after iterating through the dataset a second time, Cluster 2 contained the points [5.5, 4.2] and [5.1, 3.8], making $\\mu_2 = (5.3, 4)$\n",
        "\n",
        "3.  At convergence, $\\mu_3 = (6.3, 3.03)$\n",
        "\n",
        "4. It only took **three** iterations for convergence (the output of the third iteration was the same as the results from the second iteration's clusters)\n"
      ],
      "metadata": {
        "id": "zeoVcP8apLrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pts = np.array([[5.9, 3.2], [4.6, 2.9], [6.2, 2.8], [4.7, 3.2], [5.5, 4.2], [5.0, 3.0], [4.9, 3.1], [6.7, 3.1], [5.1, 3.8], [6.0, 3.0]])\n",
        "def dist(p1, p2):\n",
        "    return np.sqrt(np.sum((p1 - p2)**2))\n",
        "\n",
        "centers = np.array([[4.8, 3.05], [5.3, 4], [6.2, 3.03]])\n",
        "\n",
        "for pt in pts:\n",
        "  d1 = dist(pt, centers[0])\n",
        "  d2 = dist(pt, centers[1])\n",
        "  d3 = dist(pt, centers[2])\n",
        "  dists = [d1, d2, d3]\n",
        "  print(f\"for pt {pt}\")\n",
        "  print(np.argmin(dists) + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihBRXNnEn0Y_",
        "outputId": "6080b796-d2c3-4561-b93a-ab1528d08835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for pt [5.9 3.2]\n",
            "3\n",
            "for pt [4.6 2.9]\n",
            "1\n",
            "for pt [6.2 2.8]\n",
            "3\n",
            "for pt [4.7 3.2]\n",
            "1\n",
            "for pt [5.5 4.2]\n",
            "2\n",
            "for pt [5. 3.]\n",
            "1\n",
            "for pt [4.9 3.1]\n",
            "1\n",
            "for pt [6.7 3.1]\n",
            "3\n",
            "for pt [5.1 3.8]\n",
            "2\n",
            "for pt [6. 3.]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA51CAaib0J6"
      },
      "source": [
        "**[TODO: provide your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuEyJJk8b0J6"
      },
      "source": [
        "## Part 2: Coding Problems\n",
        "\n",
        "Below is a skeleton of a PyTorch-based notebook. You will fill in the indicated `TODO` blocks. We will focus on:\n",
        "\n",
        "1. **Implementing Dropout** in a fully connected network.\n",
        "2. **Implementing k-Fold Cross-Validation** for hyperparameter selection.\n",
        "\n",
        "### **Scoring Breakdown (20 points total)**\n",
        "\n",
        "1. **Insert Dropout (5 points)**  \n",
        "   *Implementation of `nn.Dropout` in the network initialization and forward pass.*\n",
        "\n",
        "2. **Evaluate Dropout Performance (5 points)**  \n",
        "   *Show how dropout changes training/test performance (e.g., final accuracy). Provide a brief analysis.*\n",
        "\n",
        "3. **Implement k-Fold Cross-Validation (5 points)**  \n",
        "   *Write a procedure that splits the training data into $k$ folds, trains on $k-1$ folds, and validates on the remaining fold.*\n",
        "\n",
        "4. **Hyperparameter Tuning and Final Results (5 points)**  \n",
        "   *Vary hyperparameters (e.g., learning rate, hidden dimension, dropout probability), identify the best setting, and report final test accuracy.*\n",
        "\n",
        "Below is a condensed example for reference. Use (and modify) as needed, and ensure each `TODO` is addressed in your final notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "covuS5U3b0J6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03b4c4e9-4efe-411f-a116-1ef489503ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.7749\n",
            "Epoch [2/10], Loss: 1.1884\n",
            "Epoch [3/10], Loss: 0.9821\n",
            "Epoch [4/10], Loss: 0.8783\n",
            "Epoch [5/10], Loss: 0.8154\n",
            "Epoch [6/10], Loss: 0.7767\n",
            "Epoch [7/10], Loss: 0.7398\n",
            "Epoch [8/10], Loss: 0.7186\n",
            "Epoch [9/10], Loss: 0.6977\n",
            "Epoch [10/10], Loss: 0.6795\n",
            "Test Accuracy with Dropout=0.5: 78.11%\n",
            "Epoch [1/10], Loss: 0.9594\n",
            "Epoch [2/10], Loss: 0.6258\n",
            "Epoch [3/10], Loss: 0.5608\n",
            "Epoch [4/10], Loss: 0.5226\n",
            "Epoch [5/10], Loss: 0.4956\n",
            "Epoch [6/10], Loss: 0.4779\n",
            "Epoch [7/10], Loss: 0.4665\n",
            "Epoch [8/10], Loss: 0.4547\n",
            "Epoch [9/10], Loss: 0.4419\n",
            "Epoch [10/10], Loss: 0.4327\n",
            "Epoch [1/10], Loss: 0.9489\n",
            "Epoch [2/10], Loss: 0.6259\n",
            "Epoch [3/10], Loss: 0.5593\n",
            "Epoch [4/10], Loss: 0.5247\n",
            "Epoch [5/10], Loss: 0.4994\n",
            "Epoch [6/10], Loss: 0.4831\n",
            "Epoch [7/10], Loss: 0.4705\n",
            "Epoch [8/10], Loss: 0.4551\n",
            "Epoch [9/10], Loss: 0.4465\n",
            "Epoch [10/10], Loss: 0.4373\n",
            "Epoch [1/10], Loss: 0.9755\n",
            "Epoch [2/10], Loss: 0.6345\n",
            "Epoch [3/10], Loss: 0.5625\n",
            "Epoch [4/10], Loss: 0.5245\n",
            "Epoch [5/10], Loss: 0.5007\n",
            "Epoch [6/10], Loss: 0.4821\n",
            "Epoch [7/10], Loss: 0.4664\n",
            "Epoch [8/10], Loss: 0.4528\n",
            "Epoch [9/10], Loss: 0.4440\n",
            "Epoch [10/10], Loss: 0.4357\n",
            "Epoch [1/10], Loss: 0.9390\n",
            "Epoch [2/10], Loss: 0.6216\n",
            "Epoch [3/10], Loss: 0.5564\n",
            "Epoch [4/10], Loss: 0.5181\n",
            "Epoch [5/10], Loss: 0.4945\n",
            "Epoch [6/10], Loss: 0.4741\n",
            "Epoch [7/10], Loss: 0.4607\n",
            "Epoch [8/10], Loss: 0.4502\n",
            "Epoch [9/10], Loss: 0.4381\n",
            "Epoch [10/10], Loss: 0.4288\n",
            "Epoch [1/10], Loss: 0.9528\n",
            "Epoch [2/10], Loss: 0.6173\n",
            "Epoch [3/10], Loss: 0.5525\n",
            "Epoch [4/10], Loss: 0.5172\n",
            "Epoch [5/10], Loss: 0.4899\n",
            "Epoch [6/10], Loss: 0.4739\n",
            "Epoch [7/10], Loss: 0.4582\n",
            "Epoch [8/10], Loss: 0.4462\n",
            "Epoch [9/10], Loss: 0.4399\n",
            "Epoch [10/10], Loss: 0.4291\n",
            "Epoch [1/10], Loss: 1.0780\n",
            "Epoch [2/10], Loss: 0.7185\n",
            "Epoch [3/10], Loss: 0.6382\n",
            "Epoch [4/10], Loss: 0.6000\n",
            "Epoch [5/10], Loss: 0.5686\n",
            "Epoch [6/10], Loss: 0.5510\n",
            "Epoch [7/10], Loss: 0.5362\n",
            "Epoch [8/10], Loss: 0.5233\n",
            "Epoch [9/10], Loss: 0.5114\n",
            "Epoch [10/10], Loss: 0.5032\n",
            "Epoch [1/10], Loss: 1.0746\n",
            "Epoch [2/10], Loss: 0.7116\n",
            "Epoch [3/10], Loss: 0.6359\n",
            "Epoch [4/10], Loss: 0.5941\n",
            "Epoch [5/10], Loss: 0.5697\n",
            "Epoch [6/10], Loss: 0.5482\n",
            "Epoch [7/10], Loss: 0.5355\n",
            "Epoch [8/10], Loss: 0.5188\n",
            "Epoch [9/10], Loss: 0.5062\n",
            "Epoch [10/10], Loss: 0.5000\n",
            "Epoch [1/10], Loss: 1.0863\n",
            "Epoch [2/10], Loss: 0.7177\n",
            "Epoch [3/10], Loss: 0.6414\n",
            "Epoch [4/10], Loss: 0.5987\n",
            "Epoch [5/10], Loss: 0.5676\n",
            "Epoch [6/10], Loss: 0.5466\n",
            "Epoch [7/10], Loss: 0.5328\n",
            "Epoch [8/10], Loss: 0.5199\n",
            "Epoch [9/10], Loss: 0.5078\n",
            "Epoch [10/10], Loss: 0.5005\n",
            "Epoch [1/10], Loss: 1.1037\n",
            "Epoch [2/10], Loss: 0.7208\n",
            "Epoch [3/10], Loss: 0.6428\n",
            "Epoch [4/10], Loss: 0.5968\n",
            "Epoch [5/10], Loss: 0.5712\n",
            "Epoch [6/10], Loss: 0.5464\n",
            "Epoch [7/10], Loss: 0.5363\n",
            "Epoch [8/10], Loss: 0.5186\n",
            "Epoch [9/10], Loss: 0.5063\n",
            "Epoch [10/10], Loss: 0.4983\n",
            "Epoch [1/10], Loss: 1.1061\n",
            "Epoch [2/10], Loss: 0.7338\n",
            "Epoch [3/10], Loss: 0.6520\n",
            "Epoch [4/10], Loss: 0.6095\n",
            "Epoch [5/10], Loss: 0.5782\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-77730d69f499>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0;34m'dropout_prob'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     }\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleFCNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameter_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleFCNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;31m# ... (train on entire train_dataset) ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-77730d69f499>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model_class, train_dataset, k, param_grid)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mfold_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-77730d69f499>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import itertools\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, RandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "##################################\n",
        "# 1. Data Loading and Transforms #\n",
        "##################################\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "############################################\n",
        "# 2. Model Definition + Dropout (5 points) #\n",
        "############################################\n",
        "class SimpleFCNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10, dropout_prob=0.5):\n",
        "        super(SimpleFCNetwork, self).__init__()\n",
        "\n",
        "\n",
        "        # 2-layer fully-connected\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # TODO (Insert Dropout layer here) - (3 points)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # TODO (Apply dropout here) - (2 points)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "##############################\n",
        "# 3. Training and Evaluation #\n",
        "##############################\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            model.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Example usage (for demonstration; you will do more thorough experiments):\n",
        "model = SimpleFCNetwork(dropout_prob=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "model = train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy with Dropout=0.5: {test_accuracy:.2f}%\")\n",
        "\n",
        "#######################################\n",
        "# 4. k-Fold Cross-Validation (5 points)\n",
        "#######################################\n",
        "def cross_validate(model_class, train_dataset, k=5, param_grid=None):\n",
        "    \"\"\"\n",
        "    param_grid is a dict of hyperparameters, e.g.:\n",
        "    {\n",
        "      'lr': [1e-2, 1e-3, 1e-4],\n",
        "      'hidden_dim': [64, 128, 256],\n",
        "      'dropout_prob': [0.25, 0.5]\n",
        "    }\n",
        "    \"\"\"\n",
        "    best_params = None\n",
        "    best_accuracy = 0.0\n",
        "    #accs = []\n",
        "\n",
        "    # TODO:\n",
        "    # (1) Shuffle and split 'train_dataset' into k folds\n",
        "    # (2) For each param combination:\n",
        "    #       For each fold:\n",
        "    #           Train on k-1 folds, validate on the remaining fold\n",
        "    #           Accumulate validation accuracy\n",
        "    #       Average the accuracy across folds\n",
        "    #       Track the best param combination\n",
        "\n",
        "\n",
        "    # split()  method generate indices to split data into training and test set.\n",
        "\n",
        "    param_combos = []\n",
        "    for lr in param_grid['lr']:\n",
        "      for hidden_dim in param_grid['hidden_dim']:\n",
        "        for dropout_prob in param_grid['dropout_prob']:\n",
        "          param_combos.append({'lr':lr, 'hidden_dim':hidden_dim, 'dropout_prob':dropout_prob})\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    dataset_indices = list(range(len(train_dataset)))\n",
        "    for param_id, params in enumerate(param_combos):\n",
        "      fold_scores = []\n",
        "      for fold, (train_idx, val_idx) in enumerate(kf.split(dataset_indices)):\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "        val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_idx))\n",
        "\n",
        "        model = model_class(hidden_dim=params['hidden_dim'], dropout_prob=params['dropout_prob'])\n",
        "        optimizer = optim.SGD(model.parameters(), lr=params['lr'])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        model = train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "        acc = evaluate_model(model, val_loader)\n",
        "        fold_scores.append(acc)\n",
        "    avg_score = np.mean(fold_scores)\n",
        "    print(f\"Params: {params}, Avg Validation Score: {avg_score:.4f}\")\n",
        "    if avg_score > best_score:\n",
        "          best_score = avg_score\n",
        "          best_params = params\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"\\nBest Params: {best_params}, Best Avg Validation Score: {best_score:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return best_params, best_score\n",
        "\n",
        "\n",
        "# After cross-validation, re-train on the full training data with best params and evaluate:\n",
        "parameter_grid = {\n",
        "      'lr': [1e-2, 1e-3, 1e-4],\n",
        "      'hidden_dim': [64, 128, 256],\n",
        "      'dropout_prob': [0.25, 0.5]\n",
        "    }\n",
        "best_params, best_val_acc = cross_validate(SimpleFCNetwork, train_dataset, k=5, param_grid=parameter_grid)\n",
        "final_model = SimpleFCNetwork(**best_params)\n",
        "# ... (train on entire train_dataset) ...\n",
        "# test_acc = evaluate_model(final_model, test_loader)\n",
        "# print(\"Final Test Accuracy =\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=params['lr'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "final_model = train_model(final_model, train_loader, criterion, optimizer, epochs=10)\n",
        "acc = evaluate_model(final_model, val_loader)\n",
        "\n",
        "print(f\"The best params were lr={best_params['lr']}, hidden_dim={best_params['hidden_dim']}, dropout_rate={best_params['dropout_rate']}\")\n",
        "print(f\"This combo yielded an accuracy of {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzgiDZFzwKdv",
        "outputId": "216b6cad-d014-4660-eeb7-e9a977bd8154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters were lr=.1, hidden_dim=256, dropout_rate = .2\n",
            "This combo yielded an accuracy of .8537\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}